# LLM Service
llm:
  image: "your-llm-image:latest"  # Replace with your LLM Docker image
  replicas: 1
  port: 8080
  resources:
    limits:
      cpu: "2000m"
      memory: "4Gi"
    requests:
      cpu: "1000m"
      memory: "2Gi"
  env:
    VECTOR_DB_ENDPOINT: "http://vector-db:8081"  # Points to the vector database

# Vector Database (e.g., Weaviate)
vectorDb:
  enabled: true
  image: "vector-db-image:latest"  # Replace with your vector database Docker image
  port: 8081
  replicas: 1
  resources:
    limits:
      cpu: "1000m"
      memory: "2Gi"
    requests:
      cpu: "500m"
      memory: "1Gi"

# Ingress
ingress:
  enabled: true
  annotations: {}
  hosts:
    - host: llm-rag.local
      paths:
        - path: /
  tls: []